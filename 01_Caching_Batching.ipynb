{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4de98c4-b7f0-487d-a03b-2f878351414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a2f389-517a-4d8c-b622-5f5a6b6dcc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a3e5e-8ab9-49a1-826a-2d7008f73c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c83e4e-9e6e-4639-ac87-0ecce3eeb47d",
   "metadata": {},
   "source": [
    "# KV Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86362ca3-f80a-4b17-904f-68f03044f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The quick brown fox jumped over the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7709b-fb37-49f0-9add-2fdfd83da0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768c036d-f2e8-4771-89e8-29b57053c75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d83cd4c-105b-4415-8374-ec80a7d60228",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812db354-4b5d-4fe3-adb7-86512304dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_logits = logits[0, -1, :]\n",
    "last_logits, len(last_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626c491-7f15-4eea-ba7c-79ccf3e8ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_id = last_logits.argmax()\n",
    "next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08608a3-d32f-470a-9b18-0ff3356ceb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(next_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e9c5a-745a-4c86-9683-373d91ffd97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = torch.topk(last_logits, k=10)\n",
    "top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fcf3b7-e06d-42de-87b6-8d31dba4f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tokenizer.decode(tk) for tk in top_k.indices]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7267128f-1bb3-405f-94f6-6be52a30059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_inputs = {\n",
    "    \"input_ids\": torch.cat(\n",
    "        [inputs[\"input_ids\"], next_token_id.reshape((1, 1))],\n",
    "        dim=1\n",
    "    ),\n",
    "    \"attention_mask\": torch.cat(\n",
    "        [inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "        dim=1\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189ebce-97e3-44b5-bea9-4a9204340d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_inputs[\"input_ids\"], next_inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8db6ba-2a1c-40f8-a466-1ee2b9ad51f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_inputs[\"attention_mask\"], next_inputs[\"attention_mask\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1949bc8b-db4d-4e3b-88dd-f21e189d732e",
   "metadata": {},
   "source": [
    "## without KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2020b79-d946-47d0-aa81-c0b35ad72278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98858881-904b-43f0-ac0e-3e2c6b8a7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = []\n",
    "next_inputs = inputs\n",
    "durations_s = []\n",
    "\n",
    "for _ in range(10):\n",
    "    t0 = time.time()\n",
    "    next_token_id = generate_token(next_inputs)\n",
    "    durations_s += [time.time() - t0]\n",
    "\n",
    "    next_inputs = { \n",
    "        \"input_ids\": torch.cat(\n",
    "            [inputs[\"input_ids\"], next_token_id.reshape((1, 1))],\n",
    "            dim=1\n",
    "            ),\n",
    "        \"attention_mask\": torch.cat(\n",
    "            [inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "            dim=1\n",
    "            ),\n",
    "        }\n",
    "    next_token = tokenizer.decode(next_token_id)\n",
    "    generated_tokens.append(next_token)\n",
    "\n",
    "print(f\"소요시간: {sum(durations_s)}\")\n",
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082ea85-ba18-4557-a8a8-1c184a4b8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(durations_s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79851084-0683-425f-9d9a-9d5e8be1dcf3",
   "metadata": {},
   "source": [
    "## with KV Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52b098-cb04-4b3f-b372-c8cf71ee4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id, outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa08b1-728c-4f0b-94c2-b46645d60025",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = []\n",
    "next_inputs = inputs\n",
    "durations_cached_s = []\n",
    "\n",
    "for _ in range(10):\n",
    "    t0 = time.time()\n",
    "    next_token_id, past_key_values = generate_token_with_past(next_inputs)\n",
    "    durations_cached_s += [time.time() - t0]\n",
    "\n",
    "    next_inputs = { \n",
    "        \"input_ids\": next_token_id.reshape((1, 1)),\n",
    "        \"attention_mask\": torch.cat(\n",
    "            [next_inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "            dim=1),\n",
    "        \"past_key_values\": past_key_values,\n",
    "        }\n",
    "    next_token = tokenizer.decode(next_token_id)\n",
    "    generated_tokens.append(next_token)\n",
    "\n",
    "print(f\"소요시간: {sum(durations_cached_s)}\")\n",
    "print(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820792bb-6a66-45c5-ba12-b697929ca929",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(durations_s)\n",
    "plt.plot(durations_cached_s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389fb00f-45f4-4039-bf87-0ae32c41aeb2",
   "metadata": {},
   "source": [
    "# Batching - issues with multiple inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30141745-b6cf-4ea4-8077-568a21f9eb2b",
   "metadata": {},
   "source": [
    "## Single Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eab60c-a088-4603-926f-113f6cddcf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The quick brown fox jumped over the\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "def generate_token_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id, outputs.past_key_values\n",
    "\n",
    "def generate(inputs, max_tokens):\n",
    "    generated_tokens = []\n",
    "    next_inputs = inputs\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_id, past_key_values = generate_token_with_past(next_inputs)\n",
    "    \n",
    "        next_inputs = { \n",
    "            \"input_ids\": next_token_id.reshape((1, 1)),\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "                dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "            }\n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        generated_tokens.append(next_token)\n",
    "    return \"\".join(generated_tokens)\n",
    "    \n",
    "tokens = generate(inputs, max_tokens=10)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a93bc0-0772-4c45-bbf8-9be8373b8d88",
   "metadata": {},
   "source": [
    "## Multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d3ee39-856f-4429-897e-b3d6a702aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cadbd1f-5d23-421d-b033-ad5cd43d13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_siede = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a31e71-6ff9-49c2-b9de-45e45167e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutiple prompts of varying lengths to send to the model at once\n",
    "prompts = [\n",
    "    \"The quick brown fox jumped over the\",\n",
    "    \"The rain in Spain falls\",\n",
    "    \"What comes up must\"\n",
    "]\n",
    "# note: padding=True ensures the padding tokens\n",
    "# will be inserted into the tokenized tensors\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343ffbc-670a-4ce5-84ac-f400eeca07fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9200e72c-e528-442b-99f5-ec08d7138f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position_ids tell the transformer the ordinal position of each token in the input sequence\n",
    "# for single input inference, this is just [0, ..n]\n",
    "# for n tokens, but for batch inference, \n",
    "# we need to 0 out the padding tokens at the start of the sequence\n",
    "\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "position_ids = attention_mask.long().cumsum(-1) - 1  # 모든항에서 1씩 빼고\n",
    "print(position_ids)\n",
    "position_ids.masked_fill_(attention_mask == 0, 1)  # 어텐션마스크가 0인 곳은 1로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c6aae-b036-4176-8af8-635028bc987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as before, but include the position_ids\n",
    "with torch.no_grad():\n",
    "    outputs = model(position_ids=position_ids, **inputs)\n",
    "logits = outputs.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43080f58-8382-418d-9b4d-3c93275abfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_logits = logits[:, -1, :]\n",
    "last_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c3edc8-5fc1-4c1f-9604-05745f9c9634",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_ids = last_logits.argmax(dim=1)\n",
    "next_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b2c62-10d3-4b87-9e31-ab52a3c43c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "next_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b6c3e8-7904-4448-8935-4d57002c2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids, outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae043ff-6894-4cda-82d3-d13c6abbc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(inputs, max_tokens):\n",
    "    generated_tokens = [\n",
    "        [] for _ in range(inputs[\"input_ids\"].shape[0])\n",
    "    ]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "        }\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        next_token_ids, past_key_values = generate_batch_tokens_with_past(next_inputs)\n",
    "    \n",
    "        next_inputs = { \n",
    "            \"input_ids\": next_token_ids.reshape((-1, 1)),\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:,-1].unsqueeze(-1) + 1,\n",
    "            \"attention_mask\": torch.cat([\n",
    "                next_inputs[\"attention_mask\"], \n",
    "                torch.ones((next_token_ids.shape[0], 1))\n",
    "            ], dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "            }\n",
    "        next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            generated_tokens[i].append(token)\n",
    "    return [\"\".join(tokens) for tokens in generated_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af38f7-4cc5-4e26-a36c-eb632de2080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = generate_batch(inputs, max_tokens=10)\n",
    "generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b194e3-42b3-4ce4-a011-88406133df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, generated in zip(prompts, generated_tokens):\n",
    "    print(prompt, f\"\\x1b[31m{generated}\\x1b[0m\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16136129-2c5b-4e1e-bcf7-f72d4b07393b",
   "metadata": {},
   "source": [
    "## Throughput vs. Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7774cf1-0fbd-4194-b6f0-dacd0811094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contants\n",
    "max_tokens = 10\n",
    "\n",
    "# observations\n",
    "durations = []\n",
    "throughputs = []\n",
    "latencies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba7b7f-b518-4aa2-87bc-2c49cc26adc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [2**p for p in range(8)]\n",
    "batch_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354921fd-ec75-48a5-803c-b7242865275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    print(f\"bs: {batch_size}\")\n",
    "\n",
    "    # generate tokens for batch and record duration\n",
    "    t0 = time.time()\n",
    "    batch_prompts = [\n",
    "        prompts[i % len(prompts)] for i in range(batch_size)\n",
    "    ]\n",
    "    inputs = tokenizer(\n",
    "        batch_prompts, padding=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    generated_tokens = generate_batch(inputs, max_tokens=max_tokens)\n",
    "    duration_s = time.time() - t0\n",
    "\n",
    "    ntokens = batch_size * max_tokens\n",
    "    throughput = ntokens / duration_s\n",
    "    avg_latency = duration_s / max_tokens\n",
    "    print(f\"ntokens: {ntokens}\")\n",
    "    print(f\"duration_s: {duration_s}\")\n",
    "    print(f\"throughput: {throughput}\")\n",
    "    print(f\"avg_latency: {avg_latency}\")\n",
    "    print()\n",
    "\n",
    "    durations.append(duration_s)\n",
    "    throughputs.append(throughput)\n",
    "    latencies.append(avg_latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dca8d2-33bb-4771-8006-f67b7812476b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_plot(x, y1, y2, x_label, y1_label, y2_label):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel(x_label)\n",
    "    ax1.set_ylabel(y1_label, color=color)\n",
    "    ax1.plot(x, y1, color = color)\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    ax1.set_xscale('log', base=2)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel(y2_label, color=color)\n",
    "    ax2.plot(x, y2, color=color)\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ea71a-31ef-451d-afe2-6144dcf4a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_plot(\n",
    "    batch_sizes,\n",
    "    throughputs,\n",
    "    latencies,\n",
    "    \"Batch Size\",\n",
    "    \"Throughput\",\n",
    "    \"Latency\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1f7b1-68a7-4b11-aa25-5b4aef1529c3",
   "metadata": {},
   "source": [
    "# Continuous Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b5f7f-55da-40f1-ba54-bfbe14a5b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184b4e7f-a6b1-4a33-bca7-e6785317f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_siede = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ba0e5-41f2-465d-8ff9-ef202e5ca656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutiple prompts of varying lengths to send to the model at once\n",
    "prompts = [\n",
    "    \"The quick brown fox jumped over the\",\n",
    "    \"The rain in Spain falls\",\n",
    "    \"What comes up must\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b792a-9fcc-4173-87bd-df68815e824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids, outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2098e5e-abea-4315-8d3a-47445158f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(inputs, max_tokens):\n",
    "    generated_tokens = [\n",
    "        [] for _ in range(inputs[\"input_ids\"].shape[0])\n",
    "    ]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "        }\n",
    "    \n",
    "    for _ in range(max_tokens):\n",
    "        next_token_ids, past_key_values = generate_batch_tokens_with_past(next_inputs)\n",
    "    \n",
    "        next_inputs = { \n",
    "            \"input_ids\": next_token_ids.reshape((-1, 1)),\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:,-1].unsqueeze(-1) + 1,\n",
    "            \"attention_mask\": torch.cat([\n",
    "                next_inputs[\"attention_mask\"], \n",
    "                torch.ones((next_token_ids.shape[0], 1))\n",
    "            ], dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "            }\n",
    "        next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            generated_tokens[i].append(token)\n",
    "    return [\"\".join(tokens) for tokens in generated_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e910499-2eaf-4623-83f6-4bdb5a65f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 32\n",
    "batch_size = 8\n",
    "\n",
    "# requests waiting to be processed\n",
    "# requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[0], 100 if i % batch_size == 0 else 10)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "\n",
    "batches = [\n",
    "    request_queue[i:i + batch_size]\n",
    "    for i in range(0, len(request_queue), batch_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64a0d2-d3b7-4ed5-b3c5-dbf2ea6a85d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_queue[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6700745-ce9a-441f-9ada-e8bcff9a6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5ee26-85ab-48c7-97ec-a2b82e9c275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9b4c4-9286-4967-9a9c-1f1ee0daf40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate tokens for all batches and record duration\n",
    "\n",
    "t0 = time.time()\n",
    "with tqdm(total=len(batches), desc=f\"bs: {batch_size}\") as pbar:\n",
    "    for i, batch in enumerate(batches):\n",
    "        # to accomplish all the requests with out current implementation, we take the max of all the tokens to generate among the requests\n",
    "        batch_max_tokens = [b[1] for b in batch]\n",
    "        max_tokens = max(batch_max_tokens)\n",
    "        pbar.set_postfix({'max_tokens': max_tokens})\n",
    "\n",
    "        batch_prompts = [b[0] for b in batch]\n",
    "        inputs = tokenizer(\n",
    "            batch_prompts, padding=True, return_tensors=\"pt\")\n",
    "        generate_batch(inputs, max_tokens=max_tokens)\n",
    "\n",
    "        pbar.update(1)\n",
    "duration_s = time.time() - t0\n",
    "duration_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c21bdd-c455-4af0-99b7-14058cce6f67",
   "metadata": {},
   "source": [
    "## 개선해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcedd75-e1b1-47f1-8ac5-2b21fabe0d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "from helpers import init_batch, generate_next_token\n",
    "from helpers import merge_batches, filter_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd8f659-1271-4b52-aa6a-f9297a5ef27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# constants\n",
    "queue_size = 32\n",
    "batch_size = 8\n",
    "\n",
    "# requests waiting to be processed\n",
    "# requests are tuples (prompt, max_tokens)\n",
    "request_queue = [\n",
    "    (prompts[0], 100 if i % batch_size == 0 else 10)\n",
    "    for i in range(queue_size)\n",
    "]\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "with tqdm(total=len(batches), desc=f\"bs: {batch_size}\") as pbar:\n",
    "    # first, let's seed the initial cached_batch\n",
    "    # with the first 'batch_size' input\n",
    "    # and run the initial prefill step\n",
    "    batch = init_batch(request_queue[:batch_size])\n",
    "    cached_batch = generate_next_token(batch)\n",
    "    request_queue = request_queue[batch_size:]\n",
    "\n",
    "    # continue until both the request queue is fully drained and every input within the cached_batch has completed generation\n",
    "    while (\n",
    "        len(request_queue) > 0 or\n",
    "        cached_batch['input_ids'].size(0) > 0\n",
    "    ):\n",
    "        batch_capacity = (\n",
    "            batch_size - cached_batch[\"input_ids\"].size(0)\n",
    "        )\n",
    "        if batch_capacity > 0 and len(request_queue) > 0:\n",
    "            # prefill\n",
    "            new_batch = init_batch(request_queue[:batch_capacity])\n",
    "            new_batch = generate_next_token(new_batch)\n",
    "            request_queue = request_queue[batch_capacity:]\n",
    "\n",
    "            # merge\n",
    "            cached_batch = merge_batches(cached_batch, new_batch)\n",
    "\n",
    "        # decode\n",
    "        cached_batch = generate_next_token(cached_batch)\n",
    "\n",
    "        # remove any inputs that have finished generation\n",
    "        cached_batch, removed_indices = filter_batch(cached_batch)\n",
    "        pbar.update(len(removed_indices))\n",
    "    \n",
    "duration_s = time.time() - t0\n",
    "duration_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10abe88f-5bc8-4a9d-86a3-ec29b9945001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
