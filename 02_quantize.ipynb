{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33247219-2aa2-4423-9939-e0f223b40042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71dc2a2d-6963-4497-b274-cb7dcfcfab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_token_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id, outputs.past_key_values\n",
    "\n",
    "def generate(inputs, max_tokens):\n",
    "    generated_tokens = []\n",
    "    next_inputs = inputs\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_id, past_key_values = generate_token_with_past(next_inputs)\n",
    "    \n",
    "        next_inputs = { \n",
    "            \"input_ids\": next_token_id.reshape((1, 1)),\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "                dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "            }\n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        generated_tokens.append(next_token)\n",
    "    return \"\".join(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bef737d-0c30-471a-836d-788c21dd98f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433929f5-dbf5-4c6e-b015-44277c946794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_siede = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcc97e66-c245-4d4c-b05a-c25a9d904100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<property object at 0x0000020DCC466F20>\n",
      "<property object at 0x0000020DB1543330>\n"
     ]
    }
   ],
   "source": [
    "# fit dtype post quantization to \"pretend\" to be fp32\n",
    "def get_float32_dtype(self):\n",
    "    return torch.float32\n",
    "print(GPT2Model.dtype)\n",
    "GPT2Model.dtype = property(get_float32_dtype)\n",
    "print(GPT2Model.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04873782-637b-4c39-8355-581c0ca42088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510342192"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206d8df-90c4-4b9c-939b-26f6093bec50",
   "metadata": {},
   "source": [
    "# Qauntize and Dequantize Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d40da827-63d7-47c3-85ac-7e6c48d9309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(t):\n",
    "    # obtain range of values in the tensor to map between 0 and 255\n",
    "    min_val, max_val = t.min(), t.max()\n",
    "\n",
    "    # determine the \"zero-point\" or value in the tensor to map to 0\n",
    "    scale = (max_val - min_val)/255\n",
    "    zero_point = min_val\n",
    "\n",
    "    # quantize and clamp to ensure we are in [0, 255]\n",
    "    t_quant = (t - zero_point) / scale\n",
    "    t_quant = torch.clamp(t_quant, min=0, max=255)\n",
    "\n",
    "    # keep track of scale and zero_point for reversing quantization\n",
    "    state = (scale, zero_point)\n",
    "\n",
    "    # cast to uint8 and return \n",
    "    t_quant = t_quant.type(torch.uint8)\n",
    "    return t_quant, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15cf48a1-8295-4a0d-b23e-55e6b68838ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
       "         [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
       "         [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
       "         ...,\n",
       "         [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
       "         [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
       "         [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]]),\n",
       " torch.Size([768, 2304]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original t\n",
    "t = model.transformer.h[0].attn.c_attn.weight.data\n",
    "t, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3fca28-9c7d-4024-8cf1-de955b902a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[107, 116, 124,  ..., 130, 125, 129],\n",
       "         [132, 135, 139,  ..., 126, 128, 127],\n",
       "         [128, 131, 145,  ..., 133, 130, 127],\n",
       "         ...,\n",
       "         [116, 127, 137,  ..., 129, 126, 130],\n",
       "         [135, 138, 133,  ..., 129, 126, 126],\n",
       "         [110, 119, 117,  ..., 128, 128, 129]], dtype=torch.uint8),\n",
       " tensor(0, dtype=torch.uint8),\n",
       " tensor(255, dtype=torch.uint8),\n",
       " (tensor(0.0221), tensor(-2.8436)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantize t\n",
    "t_q, state = quantize(t)\n",
    "t_q, t_q.min(), t_q.max(), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28098bb6-5abf-43c2-88b2-8ed5bc748b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequantize(t, state):\n",
    "    scale, zero_point = state\n",
    "    return t.to(torch.float32) * scale + zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0448ac60-efd1-44fe-aa85-095d373ef2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4774, -0.2783, -0.1014,  ...,  0.0313, -0.0793,  0.0092],\n",
       "        [ 0.0755,  0.1419,  0.2303,  ..., -0.0572, -0.0129, -0.0351],\n",
       "        [-0.0129,  0.0534,  0.3630,  ...,  0.0976,  0.0313, -0.0351],\n",
       "        ...,\n",
       "        [-0.2783, -0.0351,  0.1861,  ...,  0.0092, -0.0572,  0.0313],\n",
       "        [ 0.1419,  0.2082,  0.0976,  ...,  0.0092, -0.0572, -0.0572],\n",
       "        [-0.4110, -0.2120, -0.2562,  ..., -0.0129, -0.0129,  0.0092]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dequantize t  (퀀타이즈 전과 완전히 동일하지는 않다.)\n",
    "t_rev = dequantize(t_q, state)\n",
    "t_rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbd27ff6-8b7d-4be5-b39d-68b7f6acfccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0035, 0.0170, 0.0036,  ..., 0.0200, 0.0209, 0.0158],\n",
       "        [0.0119, 0.0055, 0.0084,  ..., 0.0046, 0.0017, 0.0195],\n",
       "        [0.0168, 0.0161, 0.0038,  ..., 0.0167, 0.0050, 0.0032],\n",
       "        ...,\n",
       "        [0.0191, 0.0187, 0.0131,  ..., 0.0004, 0.0056, 0.0006],\n",
       "        [0.0098, 0.0088, 0.0067,  ..., 0.0202, 0.0143, 0.0097],\n",
       "        [0.0010, 0.0196, 0.0162,  ..., 0.0084, 0.0199, 0.0107]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show errors (퀀타이즈 전 후 값의 차이)\n",
    "torch.abs(t - t_rev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504ece2-d76f-47b1-b7c2-e604cf81bb87",
   "metadata": {},
   "source": [
    "# Apply Quantization to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "297987ab-5225-4643-b616-bf999873558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bprompt = \"The quick brown fox jumped over the\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "def generate_token_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id, outputs.past_key_values\n",
    "\n",
    "def generate(inputs, max_tokens):\n",
    "    generated_tokens = []\n",
    "    next_inputs = inputs\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_id, past_key_values = generate_token_with_past(next_inputs)\n",
    "    \n",
    "        next_inputs = { \n",
    "            \"input_ids\": next_token_id.reshape((1, 1)),\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.tensor([[1]])],\n",
    "                dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "            }\n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        generated_tokens.append(next_token)\n",
    "    return \"\".join(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b3c0f53-a047-4794-b536-155a46e4a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_generate(model, tokenizer, inputs):\n",
    "    tokend_inputs = tokenizer(inputs, return_tensors='pt')\n",
    "    tokens = generate(tokend_inputs, max_tokens=10)\n",
    "    result = ''.join([inputs, tokens])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "01e2ead4-24b1-4bc6-9bb6-e0e25d491929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The qick brown fox jumped over the fence and ran to the other side of the fence'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = \"The qick brown fox jumped over the\"\n",
    "response_expected = full_generate(model, tokenizer, inputs)\n",
    "response_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0071580f-6487-44a8-828d-d518a398e7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "971f43bc-26b9-49a0-995c-21a16dc8b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quntize_model(model):\n",
    "    states = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "        param.data, state = quantize(param.data)\n",
    "        states[name] = state\n",
    "    return model, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7644efe5-ad02-4f37-81e0-b2b97b2c0b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model, states = quntize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92383f7b-dea3-4b6f-bab1-8e9e3425249e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137022768"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b79fed4-eafb-4848-b011-b027823f2455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기여도 측정\n",
    "def size_in_bytes(t):\n",
    "    return t.numel() * t.element_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f5391eb-9622-4315-9ff3-3f4b1f153331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1181"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([\n",
    "    size_in_bytes(v[0]) + size_in_bytes(v[1])\n",
    "    for v in states.values()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "838ee4ba-9ba6-48d2-ae82-48b2075278f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dequntize_model(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        state = states[name]\n",
    "        param.data = dequantize(param.data, state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "599a6d41-c71a-422e-a6cb-470bf37e8445",
   "metadata": {},
   "outputs": [],
   "source": [
    "deq_model = dequntize_model(q_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "29b701fd-ae56-4b1d-afea-c40afdda7f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510342192"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deq_model.get_memory_footprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c3d4630a-9e37-4616-a10b-3ed3b98f8ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The qick brown fox jumped over the same same same same same same same same same same'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한번 퀀타이즈후 디퀀타이즈해도.. 모델 성능에는 데미지가 있다.\n",
    "inputs = \"The qick brown fox jumped over the\"\n",
    "response_expected = full_generate(deq_model, tokenizer, inputs)\n",
    "response_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336f4c4-deb3-4cf1-bad0-a5a88bf173cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af8afa2-3138-404e-aa09-3a973c2bc0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400d65bf-f7f4-462b-9db6-9539af28a693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda50488-a58d-4ff4-b1ff-b848e422bc91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
